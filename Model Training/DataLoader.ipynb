{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7202a21-2afe-48c7-a48d-1cd30fa0a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from library import StockUniverse, FactorLibrary, MarketInfo, FileLoader\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "#from model import MASTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc9a070-8e7b-4e5d-b852-998ef1178765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>factor</th>\n",
       "      <th colspan=\"10\" halign=\"left\">factor_0</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">factor_185</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th>1101</th>\n",
       "      <th>1102</th>\n",
       "      <th>1103</th>\n",
       "      <th>1104</th>\n",
       "      <th>1108</th>\n",
       "      <th>1109</th>\n",
       "      <th>1110</th>\n",
       "      <th>1201</th>\n",
       "      <th>1203</th>\n",
       "      <th>1210</th>\n",
       "      <th>...</th>\n",
       "      <th>9944</th>\n",
       "      <th>9945</th>\n",
       "      <th>9946</th>\n",
       "      <th>9949</th>\n",
       "      <th>9950</th>\n",
       "      <th>9951</th>\n",
       "      <th>9955</th>\n",
       "      <th>9958</th>\n",
       "      <th>9960</th>\n",
       "      <th>9962</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-04-01</th>\n",
       "      <td>-0.204447</td>\n",
       "      <td>0.205286</td>\n",
       "      <td>-0.066181</td>\n",
       "      <td>0.698856</td>\n",
       "      <td>5.904405e+00</td>\n",
       "      <td>-7.561437e-01</td>\n",
       "      <td>-1.967005</td>\n",
       "      <td>3.591879</td>\n",
       "      <td>0.800305</td>\n",
       "      <td>1.369863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999935</td>\n",
       "      <td>0.993475</td>\n",
       "      <td>1.006718</td>\n",
       "      <td>0.995031</td>\n",
       "      <td>0.998105</td>\n",
       "      <td>1.000538</td>\n",
       "      <td>1.000069</td>\n",
       "      <td>1.000420</td>\n",
       "      <td>0.999717</td>\n",
       "      <td>0.998158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-06</th>\n",
       "      <td>1.677682</td>\n",
       "      <td>2.064220</td>\n",
       "      <td>1.186552</td>\n",
       "      <td>2.021478</td>\n",
       "      <td>4.580153e+00</td>\n",
       "      <td>7.532957e-01</td>\n",
       "      <td>-2.742347</td>\n",
       "      <td>1.634321</td>\n",
       "      <td>0.190114</td>\n",
       "      <td>0.514766</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000702</td>\n",
       "      <td>0.993183</td>\n",
       "      <td>1.002554</td>\n",
       "      <td>1.001527</td>\n",
       "      <td>0.993129</td>\n",
       "      <td>1.001826</td>\n",
       "      <td>1.000605</td>\n",
       "      <td>0.999211</td>\n",
       "      <td>1.000621</td>\n",
       "      <td>1.000076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-07</th>\n",
       "      <td>3.574125</td>\n",
       "      <td>6.542993</td>\n",
       "      <td>3.583062</td>\n",
       "      <td>3.932584</td>\n",
       "      <td>3.720238e+00</td>\n",
       "      <td>2.684145e+00</td>\n",
       "      <td>-1.923077</td>\n",
       "      <td>0.899101</td>\n",
       "      <td>0.720789</td>\n",
       "      <td>0.862999</td>\n",
       "      <td>...</td>\n",
       "      <td>1.003852</td>\n",
       "      <td>1.000872</td>\n",
       "      <td>0.992585</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>0.992867</td>\n",
       "      <td>1.003646</td>\n",
       "      <td>1.000810</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>1.000332</td>\n",
       "      <td>1.003496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-08</th>\n",
       "      <td>1.803156</td>\n",
       "      <td>3.897716</td>\n",
       "      <td>2.580645</td>\n",
       "      <td>3.086420</td>\n",
       "      <td>1.167883e+00</td>\n",
       "      <td>1.863354e+00</td>\n",
       "      <td>-0.388098</td>\n",
       "      <td>0.647733</td>\n",
       "      <td>0.830816</td>\n",
       "      <td>0.861837</td>\n",
       "      <td>...</td>\n",
       "      <td>1.006097</td>\n",
       "      <td>1.000800</td>\n",
       "      <td>0.997986</td>\n",
       "      <td>0.999393</td>\n",
       "      <td>0.998528</td>\n",
       "      <td>1.004831</td>\n",
       "      <td>0.998951</td>\n",
       "      <td>1.002109</td>\n",
       "      <td>1.003819</td>\n",
       "      <td>1.003078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-09</th>\n",
       "      <td>1.612903</td>\n",
       "      <td>2.657888</td>\n",
       "      <td>2.171137</td>\n",
       "      <td>2.251978</td>\n",
       "      <td>7.163324e-01</td>\n",
       "      <td>1.600985e+00</td>\n",
       "      <td>1.552393</td>\n",
       "      <td>1.045296</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>1.099490</td>\n",
       "      <td>...</td>\n",
       "      <td>1.003150</td>\n",
       "      <td>1.000016</td>\n",
       "      <td>1.003579</td>\n",
       "      <td>1.001872</td>\n",
       "      <td>1.001097</td>\n",
       "      <td>1.003353</td>\n",
       "      <td>0.998678</td>\n",
       "      <td>1.000542</td>\n",
       "      <td>1.004274</td>\n",
       "      <td>0.998203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-11</th>\n",
       "      <td>0.201748</td>\n",
       "      <td>1.637873</td>\n",
       "      <td>2.076125</td>\n",
       "      <td>0.662495</td>\n",
       "      <td>1.416853e+00</td>\n",
       "      <td>1.234568e+00</td>\n",
       "      <td>-1.020987</td>\n",
       "      <td>0.555213</td>\n",
       "      <td>2.632279</td>\n",
       "      <td>1.587874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996059</td>\n",
       "      <td>0.998892</td>\n",
       "      <td>0.995357</td>\n",
       "      <td>1.000269</td>\n",
       "      <td>0.997555</td>\n",
       "      <td>1.000148</td>\n",
       "      <td>0.998767</td>\n",
       "      <td>0.991700</td>\n",
       "      <td>1.004853</td>\n",
       "      <td>0.999544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-14</th>\n",
       "      <td>0.435949</td>\n",
       "      <td>0.178691</td>\n",
       "      <td>2.739726</td>\n",
       "      <td>1.645338</td>\n",
       "      <td>1.694915e+00</td>\n",
       "      <td>2.932193e+00</td>\n",
       "      <td>2.362205</td>\n",
       "      <td>2.342787</td>\n",
       "      <td>1.968504</td>\n",
       "      <td>3.183119</td>\n",
       "      <td>...</td>\n",
       "      <td>1.001241</td>\n",
       "      <td>1.000427</td>\n",
       "      <td>0.996200</td>\n",
       "      <td>0.997938</td>\n",
       "      <td>0.994582</td>\n",
       "      <td>0.998891</td>\n",
       "      <td>0.998162</td>\n",
       "      <td>0.990563</td>\n",
       "      <td>0.996839</td>\n",
       "      <td>1.000758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-15</th>\n",
       "      <td>-1.298269</td>\n",
       "      <td>-1.757899</td>\n",
       "      <td>-0.539447</td>\n",
       "      <td>-1.377810</td>\n",
       "      <td>7.251632e-02</td>\n",
       "      <td>1.501502e+00</td>\n",
       "      <td>0.788288</td>\n",
       "      <td>-0.972053</td>\n",
       "      <td>0.388098</td>\n",
       "      <td>1.405975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998429</td>\n",
       "      <td>1.000944</td>\n",
       "      <td>1.001255</td>\n",
       "      <td>0.995323</td>\n",
       "      <td>0.999422</td>\n",
       "      <td>1.000020</td>\n",
       "      <td>0.999618</td>\n",
       "      <td>0.994872</td>\n",
       "      <td>0.996795</td>\n",
       "      <td>1.001954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-16</th>\n",
       "      <td>-1.143241</td>\n",
       "      <td>-0.826446</td>\n",
       "      <td>-0.473934</td>\n",
       "      <td>-0.693684</td>\n",
       "      <td>-7.267442e-02</td>\n",
       "      <td>3.605769e-01</td>\n",
       "      <td>-0.113186</td>\n",
       "      <td>-1.098901</td>\n",
       "      <td>-0.438483</td>\n",
       "      <td>2.019499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999026</td>\n",
       "      <td>1.000255</td>\n",
       "      <td>1.005863</td>\n",
       "      <td>0.995549</td>\n",
       "      <td>1.001755</td>\n",
       "      <td>1.001636</td>\n",
       "      <td>1.003275</td>\n",
       "      <td>0.999744</td>\n",
       "      <td>0.999842</td>\n",
       "      <td>1.002569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-17</th>\n",
       "      <td>-1.658768</td>\n",
       "      <td>0.044763</td>\n",
       "      <td>0.473613</td>\n",
       "      <td>0.365631</td>\n",
       "      <td>-2.664535e-13</td>\n",
       "      <td>2.886580e-13</td>\n",
       "      <td>-1.352875</td>\n",
       "      <td>-0.674433</td>\n",
       "      <td>-0.438483</td>\n",
       "      <td>1.038062</td>\n",
       "      <td>...</td>\n",
       "      <td>1.002929</td>\n",
       "      <td>1.000698</td>\n",
       "      <td>1.004786</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997236</td>\n",
       "      <td>1.000412</td>\n",
       "      <td>1.001498</td>\n",
       "      <td>1.000800</td>\n",
       "      <td>0.997688</td>\n",
       "      <td>1.000966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1226 rows × 328667 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "factor      factor_0                                              \\\n",
       "ticker          1101      1102      1103      1104          1108   \n",
       "Date                                                               \n",
       "2020-04-01 -0.204447  0.205286 -0.066181  0.698856  5.904405e+00   \n",
       "2020-04-06  1.677682  2.064220  1.186552  2.021478  4.580153e+00   \n",
       "2020-04-07  3.574125  6.542993  3.583062  3.932584  3.720238e+00   \n",
       "2020-04-08  1.803156  3.897716  2.580645  3.086420  1.167883e+00   \n",
       "2020-04-09  1.612903  2.657888  2.171137  2.251978  7.163324e-01   \n",
       "...              ...       ...       ...       ...           ...   \n",
       "2025-04-11  0.201748  1.637873  2.076125  0.662495  1.416853e+00   \n",
       "2025-04-14  0.435949  0.178691  2.739726  1.645338  1.694915e+00   \n",
       "2025-04-15 -1.298269 -1.757899 -0.539447 -1.377810  7.251632e-02   \n",
       "2025-04-16 -1.143241 -0.826446 -0.473934 -0.693684 -7.267442e-02   \n",
       "2025-04-17 -1.658768  0.044763  0.473613  0.365631 -2.664535e-13   \n",
       "\n",
       "factor                                                            ...  \\\n",
       "ticker              1109      1110      1201      1203      1210  ...   \n",
       "Date                                                              ...   \n",
       "2020-04-01 -7.561437e-01 -1.967005  3.591879  0.800305  1.369863  ...   \n",
       "2020-04-06  7.532957e-01 -2.742347  1.634321  0.190114  0.514766  ...   \n",
       "2020-04-07  2.684145e+00 -1.923077  0.899101  0.720789  0.862999  ...   \n",
       "2020-04-08  1.863354e+00 -0.388098  0.647733  0.830816  0.861837  ...   \n",
       "2020-04-09  1.600985e+00  1.552393  1.045296  1.315789  1.099490  ...   \n",
       "...                  ...       ...       ...       ...       ...  ...   \n",
       "2025-04-11  1.234568e+00 -1.020987  0.555213  2.632279  1.587874  ...   \n",
       "2025-04-14  2.932193e+00  2.362205  2.342787  1.968504  3.183119  ...   \n",
       "2025-04-15  1.501502e+00  0.788288 -0.972053  0.388098  1.405975  ...   \n",
       "2025-04-16  3.605769e-01 -0.113186 -1.098901 -0.438483  2.019499  ...   \n",
       "2025-04-17  2.886580e-13 -1.352875 -0.674433 -0.438483  1.038062  ...   \n",
       "\n",
       "factor     factor_185                                                    \\\n",
       "ticker           9944      9945      9946      9949      9950      9951   \n",
       "Date                                                                      \n",
       "2020-04-01   0.999935  0.993475  1.006718  0.995031  0.998105  1.000538   \n",
       "2020-04-06   1.000702  0.993183  1.002554  1.001527  0.993129  1.001826   \n",
       "2020-04-07   1.003852  1.000872  0.992585  0.999707  0.992867  1.003646   \n",
       "2020-04-08   1.006097  1.000800  0.997986  0.999393  0.998528  1.004831   \n",
       "2020-04-09   1.003150  1.000016  1.003579  1.001872  1.001097  1.003353   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "2025-04-11   0.996059  0.998892  0.995357  1.000269  0.997555  1.000148   \n",
       "2025-04-14   1.001241  1.000427  0.996200  0.997938  0.994582  0.998891   \n",
       "2025-04-15   0.998429  1.000944  1.001255  0.995323  0.999422  1.000020   \n",
       "2025-04-16   0.999026  1.000255  1.005863  0.995549  1.001755  1.001636   \n",
       "2025-04-17   1.002929  1.000698  1.004786  1.000000  0.997236  1.000412   \n",
       "\n",
       "factor                                              \n",
       "ticker          9955      9958      9960      9962  \n",
       "Date                                                \n",
       "2020-04-01  1.000069  1.000420  0.999717  0.998158  \n",
       "2020-04-06  1.000605  0.999211  1.000621  1.000076  \n",
       "2020-04-07  1.000810  0.999887  1.000332  1.003496  \n",
       "2020-04-08  0.998951  1.002109  1.003819  1.003078  \n",
       "2020-04-09  0.998678  1.000542  1.004274  0.998203  \n",
       "...              ...       ...       ...       ...  \n",
       "2025-04-11  0.998767  0.991700  1.004853  0.999544  \n",
       "2025-04-14  0.998162  0.990563  0.996839  1.000758  \n",
       "2025-04-15  0.999618  0.994872  0.996795  1.001954  \n",
       "2025-04-16  1.003275  0.999744  0.999842  1.002569  \n",
       "2025-04-17  1.001498  1.000800  0.997688  1.000966  \n",
       "\n",
       "[1226 rows x 328667 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FactorLibrary.multi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4988cdd6-60c8-466a-9811-665c88075e80",
   "metadata": {},
   "source": [
    "# 下一步是依照TEST VALID TEST去劃分時間區段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d8471f8-b4df-4b12-97eb-3064871f613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllDayFactorDataset(Dataset):\n",
    "    def __init__(self, stock_universe='TWSE'):\n",
    "        self.multi_df = FileLoader.load(f'Y:\\因子回測_江建彰\\因子庫{stock_universe}.pkl')\n",
    "        self.adj_close_df = pd.read_feather(r'Y:\\因子回測_江建彰\\補上缺值日頻收盤價.ftr')\n",
    "        self.stock_list = self.get_stock_list(stock_universe)\n",
    "        \n",
    "        self.TPEX_df = MarketInfo.TPEX_norm()\n",
    "        self.RoR_df = (self.adj_close_df.shift(-5) - self.adj_close_df.shift(-1)) / self.adj_close_df.shift(-1)\n",
    "        self.RoR_df.dropna(axis=0, inplace=True)\n",
    "\n",
    "\n",
    "        new_ticker_list = self.multi_df.columns.get_level_values('ticker')\n",
    "        new_ticker_list = new_ticker_list[~new_ticker_list.duplicated()]\n",
    "\n",
    "        self.stock_list = new_ticker_list\n",
    "        self.RoR_df = self.RoR_df[self.stock_list]\n",
    "        # 這裡所有值都包含當天資訊所以要向後移\n",
    "        self.restrict_range()\n",
    "        self.check_validility()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        #self.tensor_factor = self.factor_to_tensor()\n",
    "        #self.tensor_market = self.TPEX_df.values\n",
    "        #self.tensor_return = self.RoR_df[self.stock_list]\n",
    "        #print(f'stock list : {len(self.stock_list)}')\n",
    "    def check_validility(self):\n",
    "        ticker_list1 = self.stock_list\n",
    "        ticker_list2 = self.RoR_df.columns\n",
    "        ticker_list3 = self.multi_df.columns.get_level_values('ticker')\n",
    "        ticker_list3 = ticker_list3[~ticker_list3.duplicated()]\n",
    "        assert len(ticker_list1)==len(ticker_list2)==len(ticker_list3)\n",
    "        \n",
    "        BOOL = True\n",
    "        for i in range(len(ticker_list1)):\n",
    "            if not (ticker_list1[i]==ticker_list2[i]==ticker_list3[i]):\n",
    "                BOOL = False\n",
    "        assert BOOL==True\n",
    "        \n",
    "\n",
    "        factor_list = self.multi_df.columns.get_level_values('factor')\n",
    "        factor_list = factor_list[~factor_list.duplicated()]\n",
    "        BOOL = True\n",
    "        for factor_name in factor_list:\n",
    "            ticker_list4 = self.multi_df.loc[ : , factor_name].columns\n",
    "            for i in range(len(ticker_list1)):\n",
    "                if ticker_list1[i]!=ticker_list4[i]:\n",
    "                    BOOL = False\n",
    "        assert BOOL==True\n",
    "        \n",
    "\n",
    "    def factor_to_tensor(self):\n",
    "        factor_num = len(self.multi_df.columns.get_level_values('factor').unique())\n",
    "        time = len(self.TPEX_df.index)\n",
    "        \n",
    "\n",
    "       \n",
    "            \n",
    "            \n",
    "                \n",
    "        \"\"\"\n",
    "        剔除因子缺失太多的股票\n",
    "        \n",
    "        new_stock_list = []\n",
    "        for idx, ticker in enumerate(self.stock_list):\n",
    "            numpy_array = self.multi_df.xs(ticker, axis=1, level='ticker').values\n",
    "            missing_ratio = np.isnan(numpy_array).sum() / (time*factor_num)\n",
    "            if(missing_ratio<=0.05):\n",
    "                new_stock_list.append(ticker)\n",
    "        self.stock_list = new_stock_list\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        轉成向量\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        factor_name = 'factor1'\n",
    "        #print(self.multi_df.loc[start_date : end_date, factor_name])\n",
    "\n",
    "        \n",
    "        stock_num = len(self.stock_list)\n",
    "        tensor_factor = np.empty((stock_num, time, factor_num))\n",
    "        for idx, ticker in enumerate(self.stock_list):\n",
    "            numpy_array = self.multi_df.xs(ticker, axis=1, level='ticker').ffill().values\n",
    "            numpy_array = np.nan_to_num(numpy_array, nan=0.0)\n",
    "            tensor_factor[idx] = numpy_array\n",
    "\n",
    "        \n",
    "        \n",
    "        return tensor_factor\n",
    "\n",
    "    \n",
    "\n",
    "    def restrict_range(self, global_start='2020-04-01', global_end='2025-04-09'):\n",
    "        self.multi_df     = self.multi_df.loc[global_start : global_end]\n",
    "        self.adj_close_df = self.adj_close_df.loc[global_start : global_end]\n",
    "        self.TPEX_df      = self.TPEX_df.loc[global_start : global_end]\n",
    "        self.RoR_df       = self.RoR_df.loc[global_start : global_end]\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_stock_list(self, stock_univserse):\n",
    "        if stock_univserse=='TWSE':\n",
    "            ticker1 = StockUniverse.TWSE() \n",
    "        elif stock_univserse=='OTC':\n",
    "            ticker1 = StockUniverse.OTC()\n",
    "        elif stock_univserse=='all':\n",
    "            ticker1 = StockUniverse.all()\n",
    "            \n",
    "        \n",
    "        ticker2 = self.multi_df.columns.get_level_values('ticker')\n",
    "        ticker3 = self.adj_close_df.columns\n",
    "        return list(set(ticker1)&set(ticker2)&set(ticker3))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c79c5ab9-b130-4acb-8908-2fe60c9eb852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorDataset(Dataset):\n",
    "    def __init__(self, stock_universe='TWSE', mode='train'):\n",
    "        #multi_df, TPEX_df, RoR_df = self.restrict_data_range(stock_universe, mode)\n",
    "        self.e = AllDayFactorDataset(stock_universe='TWSE')\n",
    "        self.stock_list = self.e.stock_list\n",
    "        multi_df, TPEX_df, RoR_df = self.restrict_data_range(stock_universe, mode)\n",
    "\n",
    "        \n",
    "        \n",
    "        self.tensor_factor = self.get_tensor_factors(multi_df)\n",
    "        #self.tesnor_factor = self.tesnor_factor.transpose(1,0,2) # (stock, time, factor) -> (time, stock, factor)\n",
    "        self.tensor_market = TPEX_df.values\n",
    "        self.tensor_return = RoR_df.values\n",
    "\n",
    "        self.tensor_factor = self.transform_to_tensor(self.tensor_factor)\n",
    "        self.tensor_market = self.transform_to_tensor(self.tensor_market)\n",
    "        self.tensor_return = self.transform_to_tensor(self.tensor_return)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        returns = self.tensor_return.clone()  # shape: [122, 887]\n",
    "        mean = returns.mean(dim=1, keepdim=True)   # 每天的均值 shape: [122, 1]\n",
    "        std = returns.std(dim=1, keepdim=True)     # 每天的標準差 shape: [122, 1]\n",
    "        # 防止除以 0\n",
    "        std = torch.where(std == 0, torch.tensor(1.0), std)\n",
    "        self.tensor_return_norm = (returns - mean) / std  # shape: [122, 887]\n",
    "\n",
    "\n",
    "        self.T = self.tensor_market.shape[0]\n",
    "        self.lookback = 8\n",
    "        self.valid_length = self.T - self.lookback\n",
    "\n",
    "        del self.e\n",
    "        gc.collect()\n",
    "        \n",
    "    def transform_to_tensor(self, numpy_array):\n",
    "        float32_np = numpy_array.astype(np.float32)\n",
    "        return torch.from_numpy(float32_np)\n",
    "\n",
    "    def get_tensor_factors(self, multi_df):\n",
    "        stock_num = len(multi_df.columns.get_level_values('ticker').unique())\n",
    "        time = len(multi_df.index)\n",
    "        factor_num = len(multi_df.columns.get_level_values('factor').unique())\n",
    "        \n",
    "        tensor_factor = np.empty((stock_num, time, factor_num))\n",
    "        for idx, ticker in enumerate(self.stock_list):\n",
    "            numpy_array = multi_df.xs(ticker, axis=1, level='ticker').values\n",
    "            tensor_factor[idx] = numpy_array\n",
    "\n",
    "        assert not np.isnan(tensor_factor).any()\n",
    "\n",
    "        return tensor_factor\n",
    "        \n",
    "\n",
    "    def restrict_data_range(self, stock_universe, mode):\n",
    "        e = AllDayFactorDataset(stock_universe)\n",
    "        train_ratio, valid_ratio, test_ratio = 0.8, 0.1, 0.1\n",
    "        total_num = len(self.e.TPEX_df)\n",
    "        train_num = int(total_num*train_ratio)\n",
    "        valid_num = int(total_num*valid_ratio)\n",
    "        test_num  = total_num - (train_num+valid_num)\n",
    "        if mode=='train':\n",
    "            start_idx, end_idx = 0, train_num\n",
    "        elif mode=='valid':\n",
    "            start_idx, end_idx = train_num, train_num + valid_num\n",
    "        elif mode=='test': \n",
    "            start_idx, end_idx = train_num + valid_num, total_num\n",
    "\n",
    "        e = AllDayFactorDataset(stock_universe)\n",
    "        return self.e.multi_df.iloc[start_idx : end_idx], \\\n",
    "               self.e.TPEX_df.iloc[start_idx : end_idx], \\\n",
    "               self.e.RoR_df.iloc[start_idx : end_idx]\n",
    "    def __len__(self):\n",
    "        return self.valid_length\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        X = self.tensor_factor[ : , idx:idx+self.lookback, : ] # (stock, time, factor)\n",
    "        M = self.tensor_market[idx+self.lookback-1, : ]\n",
    "        R = self.tensor_return[idx+self.lookback-1, : ]\n",
    "        R_norm = self.tensor_return_norm[idx+self.lookback-1, : ]\n",
    "        return X, M, R, R_norm\n",
    "        \n",
    "\n",
    "#t = FactorDataset(stock_universe='TWSE', mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d20289aa-1d7f-46ba-aaa0-43721517041a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 讀取: Y:\\因子回測_江建彰\\因子庫TWSE.pkl\n",
      "⚡ 快取使用: Y:\\因子回測_江建彰\\因子庫TWSE.pkl\n",
      "⚡ 快取使用: Y:\\因子回測_江建彰\\因子庫TWSE.pkl\n",
      "⚡ 快取使用: Y:\\因子回測_江建彰\\因子庫TWSE.pkl\n",
      "⚡ 快取使用: Y:\\因子回測_江建彰\\因子庫TWSE.pkl\n",
      "⚡ 快取使用: Y:\\因子回測_江建彰\\因子庫TWSE.pkl\n",
      "⚡ 快取使用: Y:\\因子回測_江建彰\\因子庫TWSE.pkl\n",
      "⚡ 快取使用: Y:\\因子回測_江建彰\\因子庫TWSE.pkl\n",
      "⚡ 快取使用: Y:\\因子回測_江建彰\\因子庫TWSE.pkl\n"
     ]
    }
   ],
   "source": [
    "#batch_size = 16 # 最原始\n",
    "batch_size = 16\n",
    "\n",
    "train_set = FactorDataset(stock_universe='TWSE', mode='train')\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valid_set = FactorDataset(stock_universe='TWSE', mode='valid')\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_set = FactorDataset(stock_universe='TWSE', mode='test')\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e85c29cd-1ee2-413e-b44e-e435e2c8d56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([976, 887])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.tensor_return.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc77d9c9-4408-435d-bf33-25cfb740cf6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([976, 887])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.tensor_return_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15003a2c-5cea-47f8-98e6-e0d95c6d29ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc450005-6a76-4791-9954-147acd719ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1e8a386-3d84-4346-abe7-2b7e4bd0679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from flash_attn import flash_attn_func\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MarketGuidedGating(nn.Module):\n",
    "    def __init__(self, market_dim, feature_dim, beta=5):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(market_dim, feature_dim)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)  # 加這行\n",
    "        nn.init.zeros_(self.fc.bias)             # 初始化 bias 為 0\n",
    "        self.beta = beta\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "    def forward(self, x, m):\n",
    "        alpha = self.feature_dim * F.softmax(self.fc(m) / self.beta, dim=-1)\n",
    "        #print(\"fc weight max:\", self.fc.weight.max().item(), \"min:\", self.fc.weight.min().item())\n",
    "        #print(self.fc(m))\n",
    "        #print(m.max(), m.min())\n",
    "        #print(torch.isnan(m.any()))\n",
    "        #print(torch.isnan(self.fc(m)).any())\n",
    "        \n",
    "        #print(F.softmax(self.fc(m) / self.beta, dim=-1).sum(axis=1))\n",
    "        #print(alpha.shape)\n",
    "        #print(f'alpha : {alpha}')\n",
    "        \n",
    "        return x * alpha  # Hadamard product\n",
    "\n",
    "class IntraStockEncoder(nn.Module):\n",
    "    def __init__(self, feature_dim, embed_dim=256, nhead=4, max_len=60):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(feature_dim, embed_dim)#\n",
    "        self.register_buffer('pos_encoder', self._get_sinusoid_encoding_table(max_len, embed_dim))#\n",
    "        self.encoder_norm = nn.LayerNorm(embed_dim)#\n",
    "        \n",
    "        self.embed_dim = embed_dim#\n",
    "        self.nhead = nhead#\n",
    "        self.head_dim = embed_dim // nhead#\n",
    "        assert self.head_dim * nhead == embed_dim, \"embed_dim must be divisible by nhead\"#\n",
    "        \n",
    "        \n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim) #\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, time, _ = x.shape#\n",
    "        x = self.input_proj(x)#\n",
    "        #print(f'x.shape : {x.shape}')\n",
    "        x = x + self.pos_encoder[:time, :].unsqueeze(0)#\n",
    "        x = self.encoder_norm(x)#\n",
    "\n",
    "        # QKV projection\n",
    "        qkv = self.qkv_proj(x)  # (batch, time, 3*embed_dim) #\n",
    "        qkv = qkv.view(batch, time, 3, self.nhead, self.head_dim) #\n",
    "        q, k, v = qkv.unbind(dim=2)  # (batch, time, nhead, head_dim) #\n",
    "\n",
    "        # Flash Attention\n",
    "        attn_out = flash_attn_func(q, k, v, dropout_p=0.0, causal=False)  # (batch, time, nhead, head_dim)#\n",
    "        attn_out = attn_out.view(batch, time, self.embed_dim)\n",
    "\n",
    "        # Residual + Norm\n",
    "        x = self.norm1(attn_out + x)\n",
    "\n",
    "        # Feed Forward\n",
    "        ffn_out = self.ffn(x)\n",
    "        out = self.norm2(ffn_out + x)  # 注意這裡是再次 residual+norm\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _get_sinusoid_encoding_table(self, seq_len, d_model):\n",
    "        position = torch.arange(seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n",
    "\n",
    "class InterStockAggregator(nn.Module):\n",
    "    def __init__(self, embed_dim=256, nhead=2):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.nhead = nhead\n",
    "        self.head_dim = embed_dim // nhead\n",
    "        assert self.head_dim * nhead == embed_dim, \"embed_dim must be divisible by nhead\"\n",
    "\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, stocks, time, embed_dim = x.shape#\n",
    "\n",
    "        x_reshaped = x.permute(0, 2, 1, 3).reshape(batch * time, stocks, embed_dim)#\n",
    "\n",
    "        qkv = self.qkv_proj(x_reshaped)\n",
    "        qkv = qkv.view(batch * time, stocks, 3, self.nhead, self.head_dim)\n",
    "        q, k, v = qkv.unbind(dim=2)\n",
    "\n",
    "        attn_out = flash_attn_func(q, k, v, dropout_p=0.0, causal=False)\n",
    "        attn_out = attn_out.reshape(batch * time, stocks, embed_dim)\n",
    "\n",
    "        x_attn = self.norm1(attn_out + x_reshaped)\n",
    "        ffn_out = self.ffn(x_attn)\n",
    "        out = self.norm2(ffn_out + x_attn)\n",
    "\n",
    "        out = out.view(batch, time, stocks, embed_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        return out\n",
    "\n",
    "class TemporalAggregator(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.w_lambda = nn.Parameter(torch.randn(embed_dim, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = x[:, :, -1, :]\n",
    "        scores = torch.einsum('bstf,fd,bsd->bst', x, self.w_lambda, query)\n",
    "        weights = F.softmax(scores, dim=2) \n",
    "        output = torch.einsum('bst,bstf->bsf', weights, x)\n",
    "        return output\n",
    "\n",
    "\n",
    "#def __init__(self, market_dim, feature_dim, embed_dim=256, nhead1=4, nhead2=2, beta=5):\n",
    "class MASTER(nn.Module):\n",
    "    def __init__(self, market_dim, feature_dim, embed_dim=256, nhead1=4, nhead2=2, beta=5):\n",
    "        super().__init__()\n",
    "        self.gating = MarketGuidedGating(market_dim, feature_dim, beta)\n",
    "        self.intra_encoder = IntraStockEncoder(feature_dim, embed_dim, nhead1)\n",
    "        self.inter_agg = InterStockAggregator(embed_dim, nhead2)\n",
    "        self.temporal_agg = TemporalAggregator(embed_dim)\n",
    "        self.predictor = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, x, market):\n",
    "        batch, stocks, time, features = x.shape\n",
    "\n",
    "        market_expanded = market[:, None, None, :].expand(-1, stocks, time, -1)\n",
    "\n",
    "        #print(f'market_expanded : {torch.isnan(market_expanded).any()}')\n",
    "        market_scaled = self.gating(x, market_expanded)\n",
    "        \n",
    "        #print(f'market_scaled : {market_scaled}')\n",
    "        #if torch.isnan(market_scaled).any():\n",
    "        #    print(\"⚠️ market_scaled 含 NaN\")\n",
    "\n",
    "        x_flat = market_scaled.view(batch * stocks, time, features)\n",
    "        local_embed = self.intra_encoder(x_flat)\n",
    "        \n",
    "        #print(f'intra_encoder : {local_embed}')\n",
    "        \n",
    "        local_embed = local_embed.view(batch, stocks, time, -1)\n",
    "\n",
    "        inter_embed = self.inter_agg(local_embed)\n",
    "        temporal_embed = self.temporal_agg(inter_embed)\n",
    "        out = self.predictor(temporal_embed).squeeze(-1)\n",
    "        #print(\"fc weight max:\", model.gating.fc.weight.max().item(), \"min:\", model.gating.fc.weight.min().item())\n",
    "        #print(f'torch.isnan(out).any() : {torch.isnan(out).any()}')\n",
    "        #print(f'out : {out}')\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc3fecf9-6e49-47db-ab3f-cf391635c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#model = MASTER(market_dim=21, feature_dim=186, beta=2).to(device)\n",
    "\n",
    "model = MASTER(market_dim=21, feature_dim=186, embed_dim=256, nhead1=4, nhead2=2, beta=2).to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "#optimizer = optim.AdamW(model.parameters(), lr=0.5, weight_decay=1e-4)\n",
    "#optimizer = optim.AdamW(model.parameters(), lr=0.005, weight_decay=1e-4) \n",
    "criterion = nn.MSELoss() # OR LORENTIAN DIST\n",
    "#criterion = torch.nn.HuberLoss(delta=1.0)  # delta 可調整為 0.5 ~ 2.0 視誤差範圍\n",
    "r_factor = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "57e3a1a3-1ea0-43f5-9814-cfa48cf236ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch 1 ===\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: predictor.weight\n",
      "❌ Inf in gradient of: predictor.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "Train Loss: 1.5673\n",
      "Valid Loss: 1.5807\n",
      "✅ Validation loss improved — model saved.\n",
      "📉 Current LR: 0.001\n",
      "=== Epoch 2 ===\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "Train Loss: 1.5674\n",
      "Valid Loss: 1.5807\n",
      "⚠️ No improvement. EarlyStopping counter: 1/20\n",
      "📉 Current LR: 0.001\n",
      "=== Epoch 3 ===\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ NaN in gradient of: gating.fc.weight\n",
      "❌ NaN in gradient of: gating.fc.bias\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.input_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.weight\n",
      "❌ NaN in gradient of: intra_encoder.encoder_norm.bias\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.weight\n",
      "❌ NaN in gradient of: intra_encoder.qkv_proj.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.0.bias\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.weight\n",
      "❌ NaN in gradient of: intra_encoder.ffn.2.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm1.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm1.bias\n",
      "❌ NaN in gradient of: intra_encoder.norm2.weight\n",
      "❌ NaN in gradient of: intra_encoder.norm2.bias\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.weight\n",
      "❌ NaN in gradient of: inter_agg.qkv_proj.bias\n",
      "❌ Inf in gradient of: inter_agg.qkv_proj.bias\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m         param_norm \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# L2 norm\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m         total_norm \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m param_norm\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;66;03m#print(param_norm)\u001b[39;00m\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;66;03m#print(f'torch.isnan(loss).any() : {torch.isnan(loss).any()}')\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir runs\n",
    "\n",
    "    \n",
    "scaler = GradScaler()\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.8,        # 一次砍半\n",
    "    patience=3,        # 調小一點\n",
    "    threshold=5e-4,     \n",
    "    threshold_mode='abs',\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "patience = 20           # 可以容忍多少次沒進步\n",
    "counter = 0             # 沒進步的次數\n",
    "min_delta = 1e-4        # 如果改善少於這個值，就不算進步\n",
    "\n",
    "# 初始化 TensorBoard writer\n",
    "#writer = SummaryWriter(log_dir='runs/experiment_1')\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'=== Epoch {epoch + 1} ===')\n",
    "\n",
    "    # -------- Training Phase --------\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        factors, market, returns, returns_norm = batch\n",
    "        factors, market, returns_norm = factors.to(device), market.to(device), returns_norm.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast('cuda', dtype=torch.float16):\n",
    "            logits = model(factors, market)\n",
    "            #print(f'torch.isnan(logits).any() : {torch.isnan(logits).any()}')\n",
    "            \n",
    "            loss = criterion(logits, returns_norm*r_factor)\n",
    "            #print(f'torch.isnan(loss).any() : {torch.isnan(loss).any()}')\n",
    "            #print(f'loss : {loss}')\n",
    "\n",
    "        #print(\"Scaler scale:\", scaler.get_scale())\n",
    "\n",
    "        #for name, param in model.named_parameters():\n",
    "        #    if param.grad is not None and torch.isnan(param.grad).any():\n",
    "        #        print(f\"❗ NaN in gradient of layer: {name}\")\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)  # 讓 clip_grad 和檢查能看到真實梯度\n",
    "\n",
    "\n",
    "        total_norm = 0.0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)  # L2 norm\n",
    "                total_norm += param_norm.item() ** 2\n",
    "                #print(param_norm)\n",
    "                #print(f'torch.isnan(loss).any() : {torch.isnan(loss).any()}')\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                if torch.isnan(param.grad).any():\n",
    "                    print(f\"❌ NaN in gradient of: {name}\")\n",
    "                if torch.isinf(param.grad).any():\n",
    "                    print(f\"❌ Inf in gradient of: {name}\")\n",
    "\n",
    "        \n",
    "        total_norm = total_norm ** 0.5\n",
    "        #print(f\"🎯 Gradient Norm: {total_norm:.6f}\")\n",
    "\n",
    "\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()     \n",
    "        \n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f'Train Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    # -------- Validation Phase --------\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            factors, market, returns, returns_norm = batch\n",
    "            factors, market, returns_norm = factors.to(device), market.to(device), returns_norm.to(device)\n",
    "\n",
    "            with autocast('cuda', dtype=torch.float16):\n",
    "                logits = model(factors, market)\n",
    "                loss = criterion(logits, returns_norm*r_factor)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    avg_valid_loss = valid_loss / len(valid_loader)\n",
    "    print(f'Valid Loss: {avg_valid_loss:.4f}')\n",
    "\n",
    "    \n",
    "    #writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "    #writer.add_scalar('Loss/Valid', avg_valid_loss, epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -------- Early Stopping 檢查 --------\n",
    "    if avg_valid_loss + min_delta < best_valid_loss:\n",
    "        best_valid_loss = avg_valid_loss\n",
    "        counter = 0\n",
    "        print(\"✅ Validation loss improved — model saved.\")\n",
    "        torch.save(model.state_dict(), \"MASTER_best_model_retrain.pt\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"⚠️ No improvement. EarlyStopping counter: {counter}/{patience}\")\n",
    "        if counter >= patience:\n",
    "            print(\"🛑 Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    scheduler.step(avg_valid_loss)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(f\"📉 Current LR: {param_group['lr']}\")\n",
    "\n",
    "# -------- Close TensorBoard Writer --------\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f90fcc-f4a9-4e8e-9eb6-c4ab97427296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa8091-000e-4973-bed1-611e7d70a05f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c5ba42-562e-4fac-9477-21d07ffd41b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ed3df8-82e8-4cca-9cc2-3388c2a02175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f101fe5-4ee7-4495-90d3-b30121fa7e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43bcbf3a-f513-4f26-8720-22f1db28f8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.0304\n"
     ]
    }
   ],
   "source": [
    "model = MASTER(market_dim=21, feature_dim=186, embed_dim=256, nhead1=4, nhead2=2, beta=2).to(device)\n",
    "state_dict = torch.load(\"MASTER_best_model.pt\", weights_only=True)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()  # 設定為評估模式（關閉 dropout、batchnorm）\n",
    "\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        factors, market, returns, returns_norm = batch\n",
    "        #print(factors.shape, market.shape)\n",
    "        #break\n",
    "        factors, market, returns, returns_norm = factors.to(device), market.to(device), returns.to(device), returns_norm.to(device)\n",
    "\n",
    "        with autocast('cuda', dtype=torch.float16):\n",
    "            logits = model(factors, market)\n",
    "            loss = criterion(logits, returns_norm*r_factor)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "print(f'Test Loss: {avg_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653d7a72-480c-4c76-9217-59f64d5868da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3dd7b12-959d-4d0e-9fbe-eb4b7da7f441",
   "metadata": {},
   "source": [
    "# 驗證模型好壞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "848ee821-568b-405c-a34c-6053216ddea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MASTER(market_dim=21, feature_dim=186, embed_dim=256, nhead1=4, nhead2=2, beta=2).to(device)\n",
    "state_dict = torch.load(\"MASTER_best_model.pt\", weights_only=True)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()  # 設定為評估模式（關閉 dropout、batchnorm）\n",
    "\n",
    "factors, market, returns = test_set.tensor_factor, test_set.tensor_market, test_set.tensor_return\n",
    "factors, market = factors.to(device), market.to(device)\n",
    "\n",
    "lookback = 8\n",
    "valid_length = len(market) - lookback\n",
    "\n",
    "cum = 1\n",
    "cum_list = []\n",
    "with torch.no_grad():\n",
    "    for idx in range(valid_length):\n",
    "        X = factors[ : , idx:idx+lookback, : ] # (stock, time, factor)\n",
    "        M = market[idx+lookback-1, : ]\n",
    "\n",
    "        X = X.unsqueeze(0) # (batch=1, stock, time, factor)\n",
    "        M = M.unsqueeze(0) # (batch=1, feature)\n",
    "        ret = returns[idx+lookback-1, : ]\n",
    "        #print(X.shape, M.shape, ret.shape)\n",
    "        \n",
    "        \n",
    "        with autocast('cuda', dtype=torch.float16):\n",
    "            logits = model(X, M)\n",
    "            logits = logits.squeeze()\n",
    "            logits = logits.cpu()\n",
    "            #print(logits.shape)\n",
    "        top_indices = torch.topk(logits, 30, largest=True).indices\n",
    "        bottom_indices = torch.topk(logits, 30, largest=False).indices\n",
    "\n",
    "        final_ret = (torch.mean(ret[top_indices])-torch.mean(ret[bottom_indices]))/2\n",
    "        \n",
    "        cum = cum*(1+final_ret)\n",
    "        cum_list.append(cum)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8f6dbd3-362a-4598-bab7-cc77b3452e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b9a873a870>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cum_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03df34d0-0a39-45dc-8234-125cefb1bc17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c5e4f7-6a07-4aa1-8099-4c9a320e45ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
