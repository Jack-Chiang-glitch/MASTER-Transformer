{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83cb4096-9200-455e-bb0c-f0c63cc31fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b3ec4ae-63d7-407e-ae34-f693cd46eee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "12.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ed4fa-4b00-4e2b-af2f-2b873f1fb380",
   "metadata": {},
   "source": [
    "\n",
    "# torch.cuda.is_available()如果是False\n",
    "# 那就是cuda版本與安裝的pytorch不符合\n",
    "# pip install torch只會安裝CPU版本的pytorch\n",
    "# 要在anaconda prompt輸入以下的指令，才能成功安裝正確版本且是GPU的pytorch\n",
    "# pip install torch --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31f1bee9-2f6f-4a15-9f68-985c7f2142f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketGuidedGating(nn.Module):\n",
    "    def __init__(self, market_dim, feature_dim, beta=5):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(market_dim, feature_dim)\n",
    "        self.beta = beta\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "    def forward(self, x, m):\n",
    "        alpha = self.feature_dim * F.softmax(self.fc(m) / self.beta, dim=-1)\n",
    "        return x * alpha  # Hadamard product\n",
    "\n",
    "\"\"\"\n",
    "class IntraStockEncoder(nn.Module):\n",
    "    def __init__(self, feature_dim, embed_dim, nhead):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(feature_dim, embed_dim)\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, 1, embed_dim))  # Learnable positional encoding\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=nhead)\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
    "\n",
    "    def forward(self, x):  # x: (batch, time, feature)\n",
    "        x = self.input_proj(x) + self.pos_encoder\n",
    "        x = self.transformer(x.transpose(0, 1)).transpose(0, 1)\n",
    "        return x  # (batch, time, embed_dim)\n",
    "\"\"\"\n",
    "\n",
    "class IntraStockEncoder(nn.Module):  # MAX_LEN屬於 time step 系列\n",
    "    def __init__(self, feature_dim, embed_dim=256, nhead=4, max_len=60): #檢查 embed_dim=256, nhead=4, max_len=60是否正確\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(feature_dim, embed_dim)\n",
    "        self.register_buffer('pos_encoder', self._get_sinusoid_encoding_table(max_len, embed_dim))  # fixed encoding\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)  # LN(f(x) + p)\n",
    "        #self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=nhead)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
    "\n",
    "    def forward(self, x):  # x: (batch, time, extract_proj_feature) extract_proj_feature = feature_dim (for example : Alpha 158)\n",
    "        batch, time, _ = x.shape\n",
    "        x = self.input_proj(x)                         # → (batch, time, embed_dim)\n",
    "        x = x + self.pos_encoder[:time, :].unsqueeze(0)  # broadcast: [1, time, embed_dim]\n",
    "        x = self.layer_norm(x)\n",
    "        #x = self.transformer(x.transpose(0, 1)).transpose(0, 1)\n",
    "        x = self.transformer(x) # 這裡的輸出是(batch, time, embed_dim) 同一個batch 以及同一支股票 就是h_{u,t} dim=256\n",
    "        return x  # (batch, time, embed_dim)\n",
    "\n",
    "    def _get_sinusoid_encoding_table(self, seq_len, d_model): # seq_len = MAX_LEN, d_model = embed_dim\n",
    "        position = torch.arange(seq_len, dtype=torch.float).unsqueeze(1)  # [seq_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # [d_model//2]\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) #要小心對應到的奇偶數不同會報錯，但不會報錯的原因是embed_dim = 256，是偶數\n",
    "        return pe  # [seq_len, d_model] that is to say [seq_len, embed_dim] or more precisely []\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "class InterStockAggregator(nn.Module):\n",
    "    def __init__(self, embed_dim, nhead):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=nhead, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x: (batch, stocks, time, embed_dim)\n",
    "        batch, stocks, time, embed_dim = x.shape\n",
    "        out = []\n",
    "        for t in range(time):\n",
    "            xt = x[:, :, t, :]  # (batch, stocks, embed_dim)\n",
    "            attn_out, _ = self.attn(xt, xt, xt)\n",
    "            out.append(self.ffn(attn_out))\n",
    "        return torch.stack(out, dim=2)  # (batch, stocks, time, embed_dim)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class InterStockAggregator(nn.Module):\n",
    "    def __init__(self, embed_dim=256, nhead=2): #數字要再確認一下\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=nhead, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4*embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*embed_dim, embed_dim),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):  # x: (batch, stocks, time, embed_dim)\n",
    "        batch, stocks, time, embed_dim = x.shape\n",
    "\n",
    "        # 變形為 (batch * time, stocks, embed_dim)，將時間軸攤平成 batch 維\n",
    "        # x.permute(0, 2, 1, 3) 變成 (batch, time, stocks, embed_dim)\n",
    "        x_reshaped = x.permute(0, 2, 1, 3).reshape(batch * time, stocks, embed_dim)\n",
    "\n",
    "        # MultiheadAttention: 每個時間點的所有股票做 MHA\n",
    "        # Self attention with residual + LN\n",
    "        attn_out, _ = self.attn(x_reshaped, x_reshaped, x_reshaped)  # (batch * time, stocks, embed_dim)\n",
    "        x_attn = self.norm1(attn_out + x_reshaped)\n",
    "\n",
    "        # Feed Forward with residual + LN\n",
    "        ffn_out = self.ffn(x_attn)\n",
    "        out = self.norm2(ffn_out + x_attn)\n",
    "        \n",
    "\n",
    "        # 還原形狀為 (batch, stocks, time, embed_dim)\n",
    "        out = out.view(batch, time, stocks, embed_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        return out  # (batch, stocks, time, embed_dim)\n",
    "\n",
    "\n",
    "\n",
    "class TemporalAggregator(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.w_lambda = nn.Parameter(torch.randn(embed_dim, embed_dim))\n",
    "\n",
    "    def forward(self, x):  # x: (batch, stocks, time, embed_dim)\n",
    "        query  = x[:, :, -1, :]  # (batch, stocks, embed_dim)\n",
    "        scores = torch.einsum('bstf,fd,bsd->bst', x, self.w_lambda, query)\n",
    "        weights = F.softmax(scores, dim=2)  # (batch, stocks, time)\n",
    "        output = torch.einsum('bst,bstf->bsf', weights, x)\n",
    "        return output  # (batch, stocks, embed_dim)\n",
    "\n",
    "\"\"\"\n",
    "class MASTER(nn.Module):\n",
    "    def __init__(self, market_dim, feature_dim, embed_dim=256, nhead1=4, nhead2=2, beta=5):\n",
    "        super().__init__()\n",
    "        self.gating = MarketGuidedGating(market_dim, feature_dim, beta)\n",
    "        self.intra_encoder = IntraStockEncoder(feature_dim, embed_dim, nhead1)\n",
    "        self.inter_agg = InterStockAggregator(embed_dim, nhead2)\n",
    "        self.temporal_agg = TemporalAggregator(embed_dim)\n",
    "        self.predictor = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, x, market):\n",
    "        # x: (batch, stocks, time, features), market: (batch, market_features)\n",
    "        batch, stocks, time, features = x.shape\n",
    "        market_scaled = self.gating(x.view(-1, features), market.repeat_interleave(stocks * time, dim=0))\n",
    "        x_rescaled = market_scaled.view(batch, stocks, time, features)\n",
    "\n",
    "        local_embed = []\n",
    "        for i in range(stocks):\n",
    "            local_embed.append(self.intra_encoder(x_rescaled[:, i, :, :]))\n",
    "        local_embed = torch.stack(local_embed, dim=1)  # (batch, stocks, time, embed_dim)\n",
    "\n",
    "        inter_embed = self.inter_agg(local_embed)  # (batch, stocks, time, embed_dim)\n",
    "        temporal_embed = self.temporal_agg(inter_embed)  # (batch, stocks, embed_dim)\n",
    "\n",
    "        out = self.predictor(temporal_embed).squeeze(-1)  # (batch, stocks)\n",
    "        return out\n",
    "\"\"\"\n",
    "\n",
    "class MASTER(nn.Module):\n",
    "    def __init__(self, market_dim, feature_dim, embed_dim=256, nhead1=4, nhead2=2, beta=5):\n",
    "        super().__init__()\n",
    "        self.gating = MarketGuidedGating(market_dim, feature_dim, beta)\n",
    "        self.intra_encoder = IntraStockEncoder(feature_dim, embed_dim, nhead1)\n",
    "        self.inter_agg = InterStockAggregator(embed_dim, nhead2)\n",
    "        self.temporal_agg = TemporalAggregator(embed_dim)\n",
    "        self.predictor = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, x, market):\n",
    "        # x: (batch, stocks, time, features), market: (batch, market_features)\n",
    "        batch, stocks, time, features = x.shape\n",
    "\n",
    "        # ====== Gating ======\n",
    "        #market_expanded = market.unsqueeze(1).unsqueeze(2).expand(-1, stocks, time, -1)  # (batch, stocks, time, market_features)\n",
    "        market_expanded = market[ : , None, None, : ].expand(-1, stocks, time, -1)\n",
    "        #print(f\"market expanded shape = {market_expanded.shape}\")\n",
    "        #print(f\"x = {x.shape}\")\n",
    "        market_scaled = self.gating(x, market_expanded)  # broadcasting-wise multiplication\n",
    "\n",
    "        # ====== Intra-Stock Encoder ======\n",
    "        print(f\"market_scaled shape = {market_scaled.shape}\")\n",
    "        x_flat = market_scaled.view(batch * stocks, time, features)  # (batch*stocks, time, features)\n",
    "        print(f\"x_flat shape = {x_flat.shape}\")\n",
    "        local_embed = self.intra_encoder(x_flat)  # → (batch*stocks, time, embed_dim)\n",
    "        local_embed = local_embed.view(batch, stocks, time, -1)  # → (batch, stocks, time, embed_dim)\n",
    "\n",
    "        # ====== Inter-Stock Aggregation ======\n",
    "        inter_embed = self.inter_agg(local_embed)  # (batch, stocks, time, embed_dim)\n",
    "\n",
    "        # ====== Temporal Aggregation ======\n",
    "        temporal_embed = self.temporal_agg(inter_embed)  # (batch, stocks, embed_dim)\n",
    "        print(f\"temporal shape = {temporal_embed.shape}\")\n",
    "\n",
    "        # ====== Prediction ======\n",
    "        out = self.predictor(temporal_embed).squeeze(-1)  # (batch, stocks)\n",
    "        print(self.predictor(temporal_embed).shape)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6028061-16e9-41dd-bff8-f2528f4204e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "market_scaled shape = torch.Size([8, 1822, 8, 185])\n",
      "x_flat shape = torch.Size([14576, 8, 185])\n",
      "temporal shape = torch.Size([8, 1822, 256])\n",
      "torch.Size([8, 1822, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1822])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "stocks = 1822\n",
    "time = 8\n",
    "feature_dim = 185\n",
    "market_dim = 21\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "x = torch.randn(batch_size, stocks, time, feature_dim)\n",
    "m = torch.randn(batch_size, market_dim)\n",
    "model = MASTER(market_dim = market_dim, feature_dim = feature_dim)\n",
    "x = x.to(device)\n",
    "m = m.to(device)\n",
    "model = model.to(device)\n",
    "model(x, m).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf119eb7-9009-44df-9b90-bb21ca10819f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd7da5-43e8-456a-b04d-6a1139eff4dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "695177f6-9bbe-44ab-9558-78336c5a3ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 60, 256])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 不直接使用nn.TransformerEncoder 但是MHA是現成的\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class IntraStockEncoderManual(nn.Module):\n",
    "    def __init__(self, feature_dim, embed_dim=256, nhead=4, max_len=60):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(feature_dim, embed_dim)\n",
    "        self.register_buffer('pos_encoder', self._get_sinusoid_encoding_table(max_len, embed_dim))\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=nhead, batch_first=True)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x: (batch, time, feature_dim)\n",
    "        batch, time, _ = x.shape\n",
    "\n",
    "        x = self.input_proj(x)  # (batch, time, embed_dim)\n",
    "        x = x + self.pos_encoder[:time, : ].unsqueeze(0)  # 加上位置編碼\n",
    "\n",
    "        # === Self Attention over time ===\n",
    "        residual = x\n",
    "        attn_out, _ = self.self_attn(x, x, x)  # (batch, time, embed_dim)\n",
    "        x = self.layer_norm1(attn_out + residual)  # 殘差 + LN\n",
    "\n",
    "        # === Feed Forward Network ===\n",
    "        residual = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.layer_norm2(x + residual)  # 殘差 + LN\n",
    "\n",
    "        return x  # (batch, time, embed_dim)\n",
    "\n",
    "    def _get_sinusoid_encoding_table(self, seq_len, d_model):\n",
    "        position = torch.arange(seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe  # (seq_len, d_model)\n",
    "\n",
    "ISEM = IntraStockEncoderManual(158)\n",
    "x = torch.randn(16, 60, 158)\n",
    "ISEM(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb1332b-7c2b-4311-af49-06b4a6aada09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 連 Multihead attention都是用手刻的(這是單頭的)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ManualIntraStockEncoder(nn.Module):\n",
    "    def __init__(self, feature_dim, embed_dim, nhead):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(feature_dim, embed_dim)\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.attn_out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):  # x: (batch, time, feature_dim)\n",
    "        x = self.input_proj(x)  # → (batch, time, embed_dim)\n",
    "\n",
    "        # Q, K, V (batch, time, embed_dim)\n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "\n",
    "        # 做 scaled dot-product attention（手動多頭的簡化版，單頭）\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (Q.size(-1) ** 0.5)  # (batch, time, time)\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_out = torch.matmul(attn_weights, V)  # (batch, time, embed_dim)\n",
    "\n",
    "        # 殘差 + LayerNorm\n",
    "        x = self.layernorm1(x + self.attn_out_proj(attn_out))\n",
    "\n",
    "        # FFN + 殘差 + LayerNorm\n",
    "        x = self.layernorm2(x + self.ffn(x))\n",
    "\n",
    "        return x  # shape: (batch, time, embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89807cc-32a7-4dc8-a079-3c3fc5f64b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這是自己手刻的多頭\n",
    "#這種是分成不同Q K V的，也可以直接使用一個大矩陣，再用chunk分成3等份\n",
    "\n",
    "class ManualMultiHeadIntraStockEncoder(nn.Module):\n",
    "    def __init__(self, feature_dim, embed_dim, nhead):\n",
    "        super().__init__()\n",
    "        assert embed_dim % nhead == 0, \"embed_dim 必須可以被 nhead 整除\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.nhead = nhead\n",
    "        self.head_dim = embed_dim // nhead\n",
    "\n",
    "        self.input_proj = nn.Linear(feature_dim, embed_dim)\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.attn_out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):  # x: (batch, time, feature_dim)\n",
    "        B, T, _ = x.shape\n",
    "        x = self.input_proj(x)  # → (B, T, embed_dim)\n",
    "\n",
    "        # Q, K, V linear projection   (batch, time, embed_dim) -> (batch, time, nhead, head_dim) -> (batch, nhead, time, head_dim)\n",
    "        Q = self.q_proj(x).view(B, T, self.nhead, self.head_dim).transpose(1, 2)  # (B, nhead, T, head_dim) dim1 和 dim2交換\n",
    "        K = self.k_proj(x).view(B, T, self.nhead, self.head_dim).transpose(1, 2)\n",
    "        V = self.v_proj(x).view(B, T, self.nhead, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (B, nhead, T, T)\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_out = torch.matmul(attn_weights, V)  # (B, nhead, T, head_dim)\n",
    "\n",
    "        # 合併 heads\n",
    "        attn_out = attn_out.transpose(1, 2).contiguous().view(B, T, self.embed_dim)  # → (B, T, embed_dim)\n",
    "\n",
    "        # 殘差 + LayerNorm\n",
    "        x = self.layernorm1(x + self.attn_out_proj(attn_out))\n",
    "\n",
    "        # FFN + 殘差 + LayerNorm\n",
    "        x = self.layernorm2(x + self.ffn(x))\n",
    "\n",
    "        return x  # (batch, time, embed_dim)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
